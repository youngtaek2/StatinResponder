{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-50dcnj70 because the default path (/home/youngtaek/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from boruta import BorutaPy\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "# from sklearn.metrics import plot_roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from catboost import CatBoostRegressor\n",
    "import pickle\n",
    "from colorama import Fore, Back, Style\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import Ridge\n",
    "seed = 42\n",
    "\n",
    "from sklearn import svm\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2324, 934)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path = 'PARADIGM_DB/merged_DB_at_major_revision_v4.xlsx'\n",
    "merged_df = pd.read_excel(data_path, engine='openpyxl')\n",
    "print (merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1898, 934), Unique NewID count: 443\n",
      "Test shape: (426, 934), Unique NewID count: 120\n"
     ]
    }
   ],
   "source": [
    "train_df = merged_df[merged_df[\"Site_ID\"].isin([3,9])]\n",
    "test_df = merged_df[~merged_df[\"Site_ID\"].isin([3,9])]\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Unique NewID count: {train_df['NewID'].nunique()}\")\n",
    "print(f\"Test shape: {test_df.shape}, Unique NewID count: {test_df['NewID'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of shape features: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_944588/3461878492.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df.dropna(subset=data.columns, inplace=True)\n",
      "/tmp/ipykernel_944588/3461878492.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df.dropna(subset=data.columns, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "shape_featres = [col for col in train_df.columns if 'shape' in col]\n",
    "print (\"number of shape features:\", len(shape_featres))\n",
    "\n",
    "# pickle 파일을 열기\n",
    "with open('reproducible_features_inter_intra_analysis.pkl', 'rb') as file:\n",
    "    # pickle 데이터를 읽고 역직렬화하기\n",
    "    reproducible_features = pickle.load(file)\n",
    "\n",
    "prefixed_items = ['pre_' + item for item in reproducible_features] + shape_featres\n",
    "\n",
    "inputs = train_df.filter(regex='pre_original|pre_wavelet')\n",
    "target = train_df['StatinResponder']\n",
    "data = pd.concat([inputs, target], axis=1)\n",
    "train_df.dropna(subset=data.columns, inplace=True)\n",
    "\n",
    "inputs = test_df.filter(regex='pre_original|pre_wavelet')\n",
    "target = test_df['StatinResponder']\n",
    "data = pd.concat([inputs, target], axis=1)\n",
    "test_df.dropna(subset=data.columns, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               feature  rank\n",
      "348             pre_wavelet-LHH_firstorder_TotalEnergy     1\n",
      "391  pre_wavelet-HLL_glrlm_LongRunHighGrayLevelEmph...     1\n",
      "66              pre_wavelet-HHH_firstorder_TotalEnergy     1\n",
      "392             pre_wavelet-HHL_firstorder_TotalEnergy     1\n",
      "393         pre_wavelet-HHH_firstorder_RootMeanSquared     1\n",
      "..                                                 ...   ...\n",
      "415                 pre_original_glrlm_LongRunEmphasis   628\n",
      "381  pre_original_glrlm_RunLengthNonUniformityNorma...   629\n",
      "344          pre_original_gldm_SmallDependenceEmphasis   630\n",
      "389                        pre_original_ngtdm_Busyness   631\n",
      "601                             pre_original_glcm_Imc1   632\n",
      "\n",
      "[697 rows x 2 columns]\n",
      "[0.01412709 0.01667153 0.         0.02348031 0.0163468  0.01107448\n",
      " 0.01216992 0.01311162 0.01592055 0.02113141 0.0149174  0.01640239\n",
      " 0.05793812 0.0197974  0.01234543 0.01765283 0.0457972  0.\n",
      " 0.01922678 0.02013523 0.03809552 0.04921979 0.0210906  0.01308797\n",
      " 0.02837888 0.01407621 0.0115226  0.00886095 0.01771487 0.01356775\n",
      " 0.         0.         0.         0.01454742 0.01530588 0.\n",
      " 0.         0.02143477 0.02263848 0.0212508  0.0150973  0.00945294\n",
      " 0.         0.0138879  0.02436775 0.         0.01702221 0.\n",
      " 0.02308794 0.01741947 0.01461407 0.02001566 0.01503025 0.\n",
      " 0.01377873 0.01893444 0.01052428 0.         0.02486027 0.01704661\n",
      " 0.02394143 0.         0.03055539 0.01132438]\n"
     ]
    }
   ],
   "source": [
    "target = train_df['StatinResponder']\n",
    "source = train_df[prefixed_items]\n",
    "\n",
    "# create a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=0)\n",
    "\n",
    "# create the Boruta feature selector\n",
    "boruta = BorutaPy(rf, n_estimators='auto', max_iter=30, verbose=0, random_state=seed)\n",
    "\n",
    "# fit the Boruta feature selector\n",
    "boruta.fit(source.values, target.values)\n",
    "\n",
    "feature_ranking = pd.DataFrame({'feature': source.columns, 'rank': boruta.ranking_})\n",
    "\n",
    "# Print the most important features\n",
    "print(feature_ranking.sort_values('rank'))\n",
    "\n",
    "# feature_ranking.to_excel('MajorRevision/bourta_feature_selector_feature_ranking_major_revision.xlsx', index=False)\n",
    "\n",
    "bourtaFeatures = feature_ranking[feature_ranking['rank']==1]['feature'].values.tolist()\n",
    "\n",
    "# Train an XGBoost model\n",
    "model = XGBClassifier(objective='binary:logistic', max_depth=3, learning_rate=0.01, n_estimators=200, random_state=seed)\n",
    "model.fit(source[bourtaFeatures], target)\n",
    "\n",
    "# Get the feature importances\n",
    "importances = model.feature_importances_\n",
    "print (importances)\n",
    "\n",
    "# Create a new DataFrame to store the feature importances\n",
    "feature_importances = pd.DataFrame({'Feature': source[bourtaFeatures].columns, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "# filter out rows with zero importance\n",
    "feature_importances = feature_importances[feature_importances['Importance'] != 0]\n",
    "\n",
    "# feature_importances.to_csv('./MajorRevision/feature_selection_StatinResponder_site39_major_revision.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    (LGBMClassifier(random_state=seed), {\n",
    "        'n_estimators': [30, 40, 50, 60, 70],\n",
    "        'learning_rate': [0.1, 0.5, 0.01, 0.05],\n",
    "        'max_depth': [-1, 3, 5, 10, 15, 20],\n",
    "        'num_leaves': [5, 10, 25, 30, 50],\n",
    "        'boosting_type': ['gbdt', 'dart', 'goss'],\n",
    "        'verbosity': [-1],\n",
    "        'min_child_samples': [5, 10, 20, 50],\n",
    "        'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "        'reg_lambda': [0, 0.01, 0.1, 1],\n",
    "        'feature_fraction': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "        'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    }),\n",
    "    (XGBClassifier(random_state=seed), {\n",
    "        'max_depth': [3, 5, 10, 15, 20, 30],\n",
    "        'learning_rate': [0.1, 0.5, 0.01, 0.05],\n",
    "        'n_estimators': [5, 10, 15, 20, 30, 50],\n",
    "        'subsample': [0.5, 0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.5, 0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.3, 0.5, 1],\n",
    "        'min_child_weight': [1, 3, 5, 10]\n",
    "    }),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_importances = pd.read_csv('./MajorRevision/feature_selection_StatinResponder_site39_secondary.csv')\n",
    "print (len(feature_importances))\n",
    "\n",
    "features = feature_importances['Feature'].values.tolist()\n",
    "\n",
    "# # train data\n",
    "y_train = train_df['StatinResponder']\n",
    "X_train = train_df[features]\n",
    "\n",
    "\n",
    "# # Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "y_test = test_df['StatinResponder']\n",
    "X_test = test_df[features]\n",
    "\n",
    "\n",
    "results = []  # 모든 iteration의 결과를 저장할 리스트\n",
    "\n",
    "for clf, params in classifiers:\n",
    "    print (clf.__class__.__name__)\n",
    "    best_val_score = 0\n",
    "    best_test_score = 0\n",
    "    param_grid = ParameterGrid(params)\n",
    "    for param in param_grid:\n",
    "        clf.set_params(**param)\n",
    "        clf.fit(X_train_resampled, y_train_resampled)\n",
    "        train_score = roc_auc_score(y_train_resampled, clf.predict_proba(X_train_resampled)[:, 1])\n",
    "        \n",
    "        test_score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
    "        print (train_score, test_score)\n",
    "        if test_score >= best_test_score:\n",
    "            best_test_score = test_score\n",
    "            results.append(\n",
    "                {\n",
    "                    'Classifier': clf.__class__.__name__,\n",
    "                    'Params': param,\n",
    "                    'Train_score': train_score,\n",
    "                    'Test_score': test_score\n",
    "                })\n",
    "            \n",
    "            dump(clf, './MajorRevision/{}_segment_prediction_StatinResponder_site39_secondary_{}.joblib'.format(clf.__class__.__name__, exp_name))\n",
    "            print (\"train score\", train_score, \"test score\", test_score)\n",
    "            # print (clf.__class__.__name__, \"model save\", './MajorRevision/{}_segment_prediction.joblib'.format(clf.__class__.__name__), \"score\", best_score)\n",
    "\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"./MajorRevision/{}_segment_prediction_StatinResponder_site39_secondary_{}.csv\".format(clf.__class__.__name__, exp_name), index=False)\n",
    "print(\"All experiment results saved to CSV.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radiomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
